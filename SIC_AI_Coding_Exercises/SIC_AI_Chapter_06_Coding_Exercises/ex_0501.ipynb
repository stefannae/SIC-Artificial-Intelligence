{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding Exercise #0501"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Classification with logistic regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1. Read in data and explore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data.\n",
    "data = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the description.\n",
    "print(data.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explanatory variables.\n",
    "X = data['data']\n",
    "print(data['feature_names'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Response variable.\n",
    "# Relabel such that 0 = 'benign' and 1 = malignant.\n",
    "Y = 1 - data['target']\n",
    "label = list(data['target_names'])\n",
    "label.reverse()\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the frequency table.\n",
    "ser = pd.Series(Y)\n",
    "table = ser.value_counts()\n",
    "table = table.sort_index()                                         # Has to be sorted for correct labeling.\n",
    "sns.barplot(label,table.values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2. Train and test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing.\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.4, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_train.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and predict.\n",
    "LL = LogisticRegression(solver='liblinear',max_iter=200)\n",
    "LL.fit(X_train,Y_train)\n",
    "Y_pred_test = LL.predict(X_test)                            # Out-of-sample prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix.\n",
    "conf_mat = metrics.confusion_matrix(Y_test,Y_pred_test)\n",
    "print(conf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy, Sensitivity, Specificity and Precision using the confusion matrix.\n",
    "accuracy = (conf_mat[0,0] + conf_mat[1,1])/np.sum(conf_mat)\n",
    "sensitivity = conf_mat[1,1]/(conf_mat[1,0]+conf_mat[1,1])\n",
    "specificity = conf_mat[0,0]/(conf_mat[0,0]+conf_mat[0,1])\n",
    "precision = conf_mat[1,1]/(conf_mat[0,1]+conf_mat[1,1])\n",
    "print('Accuracy    = {}'.format(np.round(accuracy,3)))\n",
    "print('Sensitvity  = {}'.format(np.round(sensitivity,3)))\n",
    "print('Specificity = {}'.format(np.round(specificity,3)))\n",
    "print('Precision   = {}'.format(np.round(precision,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative way.\n",
    "accuracy = metrics.accuracy_score(Y_test,Y_pred_test)                      # Alternative way to calculate the accuracy.\n",
    "sensitivity = metrics.recall_score(Y_test,Y_pred_test)\n",
    "precision = metrics.precision_score(Y_test,Y_pred_test)\n",
    "print('Accuracy    = {}'.format(np.round(accuracy,3)))\n",
    "print('Sensitvity  = {}'.format(np.round(sensitivity,3)))\n",
    "print('Precision   = {}'.format(np.round(precision,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3. Cutoff (threshold):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, predict the probability of Y  = 1.\n",
    "Y_pred_test_prob=LL.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One can change the cutoff at will\n",
    "cutoff = 0.7                                                      # cutoff can be a value between 0 and 1.\n",
    "Y_pred_test_val = (Y_pred_test_prob > cutoff).astype(int)\n",
    "conf_mat = metrics.confusion_matrix(Y_test,Y_pred_test_val)\n",
    "print(conf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = (conf_mat[0,0] + conf_mat[1,1])/np.sum(conf_mat)\n",
    "sensitivity = conf_mat[1,1]/(conf_mat[1,0]+conf_mat[1,1])\n",
    "specificity = conf_mat[0,0]/(conf_mat[0,0]+conf_mat[0,1])\n",
    "precision = conf_mat[1,1]/(conf_mat[0,1]+conf_mat[1,1])\n",
    "print('Accuracy    = {}'.format(np.round(accuracy,3)))\n",
    "print('Sensitvity  = {}'.format(np.round(sensitivity,3)))\n",
    "print('Specificity = {}'.format(np.round(specificity,3)))\n",
    "print('Precision   = {}'.format(np.round(precision,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4. ROC curve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize.\n",
    "cutoff_grid = np.linspace(0.0,1.0,100)\n",
    "TPR = []                                                   # True Positive Rate.\n",
    "FPR = []                                                   # False Positive Rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populate the TP and FP lists.\n",
    "for cutoff in cutoff_grid:\n",
    "    Y_pred_test_val = (Y_pred_test_prob > cutoff).astype(int)\n",
    "    conf_mat = metrics.confusion_matrix(Y_test,Y_pred_test_val)\n",
    "    sensitivity = conf_mat[1,1]/(conf_mat[1,0]+conf_mat[1,1])\n",
    "    specificity = conf_mat[0,0]/(conf_mat[0,0]+conf_mat[0,1])\n",
    "    TPR.append(sensitivity)\n",
    "    FPR.append(1-specificity)                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize.\n",
    "plt.plot(FPR,TPR,c='red',linewidth=1.0)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5. ROC curve (sklearn):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the TPR and FPR using a Scikit Learn function.\n",
    "FPR, TPR, cutoffs = metrics.roc_curve(Y_test,Y_pred_test_prob,pos_label=1)      # positive label = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize.\n",
    "plt.plot(FPR,TPR,c='red',linewidth=1.0)\n",
    "plt.xlabel('False Positive')\n",
    "plt.ylabel('True Positive')\n",
    "plt.title('ROC Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUC.\n",
    "auc = metrics.roc_auc_score(Y_test,Y_pred_test_prob)\n",
    "print('AUC  = {}'.format(np.round(auc,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
